---
title: "R Homework 2"
author: "Gabe Mancino"
date: "2/9/2018"
output:
  pdf_document: default
  html_document: default
---


Let $\theta$ denote the median of the $EXP(\beta)$ distribution; i.e. $\theta$ is the quantity that satisfies:

$$0.5 = \int_0^\theta \frac{1}{\beta}e^{-y/\beta} dy$$

Consider the problem of estimating $\theta$ from a sample $Y_1, Y_2,...,Y_n$ drawn i.i.d. from this $EXP(\beta)$ distribution.

1.  Derive $\theta$ as a function of $\beta$.  Show your work.

\textbf{Solution.}
We begin by noting that the CDF of a $Y\sim EXP(\beta)$ is defined to be
\[
F(y)=
\begin{cases}
0 &y<0\\
1-e^{-y/\beta} &0\le y
\end{cases}.
\]
Thus,
\[
\int_0^\theta \frac{1}{\beta}e^{-y/\beta} dy=1-e^{-\theta/\beta}\stackrel{set}{=}0.5,
\]
so solving for $\theta$ yields

\[
\begin{aligned}
0.5 &=1-e^{-\theta/\beta}\\
e^{-\theta/\beta} &=0.5\\
-\theta/\beta &=\ln(0.5)\\
\theta &=-\beta\ln(0.5).
\end{aligned}
\]

$\hfill\square$

2.  Let $\hat\theta_1 = \bar Y$, the sample mean.  Is $\hat\theta_1$ an unbiased estimator of $\theta$?  Fully justify your response.

\textbf{Solution.}
We begin by solving for $E(\hat{\theta_1})\equiv E(\bar{Y}),$
\[
E(\bar{Y})=E\Big(\frac{\sum\limits_{i=1}^{n}{Y_i}}{n}\Big)=\frac{1}{n}E(\sum\limits_{i=1}^{n}{Y_i})\stackrel{i.i.d.}{=}\frac{n\beta}{n}=\beta.
\]
Computing $B(\hat{\theta}_1)=E(\hat{\theta}_1)-\theta=\beta-\theta\ne0$ implies that $\hat{\theta}_1$ is biased.

$\hfill\square$

3.  Define $\hat\theta_2 = h(\bar Y)$ to be an unbiased estimator of $\theta$ that is a function of $\bar Y$.  Find $\hat{\theta}_2$.

\textbf{Solution.}
Using what we derived in Question 1 we can note that $-\beta\ln(0.5)=\theta,$ thus $\hat{\theta}_2=-\bar{Y}\ln(0.5)$ is unbiased.

To prove this: $E(\hat{\theta}_2)=-\ln(0.5)E(\bar{Y})=-\ln(0.5)\beta=\theta$ so $B(\hat{\theta}_2)=\theta-\theta=0.$

$\hfill\square$

3.  Let $\hat\theta_3$ be the median of the sample of size $n$.  Finding the bias of $\hat\theta_3$ analytically is hard!  Prove this to yourself by writing down the integral that you would need to evaluate to find $E(\hat\theta_3)$.  (You may assume $n$ is odd in your derivation.)

\textbf{Solution.}
To derive $\hat{\theta_3}$ we will assume $n$ is odd thus we will be using page 22 from our Chapter 6 notes. The median order statistic is given by $Y_{(\frac{n+1}{2})}$ which can found by substituting $\frac{n+1}{2}$ in for $j$ thus
\[
f_{Y_{(\frac{n+1}{2})}}(y)=\frac{n!}{(\frac{n+1}{2}-1)!(n-\frac{n+1}{2})!}\Big(\frac{1}{\beta}e^{-y/\beta}\Big)\Big(1-e^{-y/\beta}\Big)^{\frac{n+1}{2}-1}\Big(e^{-y/\beta}\Big)^{n-(\frac{n+1}{2})};\hspace{2cm} y>0.
\]
Thus to solve for $E(\hat{\theta_3})$ we will be computing the integral:
\[
E(\hat{\theta}_3)=\int_0^{\infty}y\cdot\frac{n!}{(\frac{n+1}{2}-1)!(n-\frac{n+1}{2})!}\Big(\frac{1}{\beta}e^{-y/\beta}\Big)\Big(1-e^{-y/\beta}\Big)^{\frac{n+1}{2}-1}\Big(e^{-y/\beta}\Big)^{n-(\frac{n+1}{2})}dy
\]
which does indeed look nasty.

$\hfill\square$

4. Use simulation studies to compare the three estimators when $\beta = 5$.  Use 100,000 replications. Fill in the table below with the outcomes of your simulation study:

\textbf{Solution.}
Consider the following code:
```{r, tidy = TRUE, tidy.opts=list(width.cutoff=60)}
theta.hats <- function(n){
  y <- rexp(n, rate = 1/5)
  ybar <- mean(y)
  theta.hat1 <- ybar
  theta.hat2 <- ybar * -log(0.5)
  theta.hat3 <- median(y)
  output <- c('ThetaHat1' = theta.hat1, 'ThetaHat2' = theta.hat2, 'ThetaHat3' = theta.hat3)
  return(output)
}
mse <- function(estimators){
  mse <- mean((estimators - (-log(0.5)*5))^2)
  return(mse)
}
#The true theta
theta <- qexp(0.5, rate = 1/5)
# n = 3
many.thetahats.n3 <- replicate(100000, theta.hats(n = 3))
mean.thetahats.n3 <- round(data.frame(apply(many.thetahats.n3, 1, mean) - theta), digits = 2)
var.thetahats.n3 <- round(data.frame(apply(many.thetahats.n3, 1, var)), digits = 2)
mse.thetahats.n3 <- round(data.frame(apply(many.thetahats.n3, 1, mse)), digits = 2)
# n = 11
many.thetahats.n11 <- replicate(100000, theta.hats(n = 11))
mean.thetahats.n11 <- round(data.frame(apply(many.thetahats.n11, 1, mean) - theta), digits = 2)
var.thetahats.n11 <- round(data.frame(apply(many.thetahats.n11, 1, var)), digits = 2)
mse.thetahats.n11 <- round(data.frame(apply(many.thetahats.n11, 1, mse)), digits = 2)
# n = 51
many.thetahats.n51 <- replicate(100000, theta.hats(n = 51))
mean.thetahats.n51 <- round(data.frame(apply(many.thetahats.n51, 1, mean) - theta), digits = 2)
var.thetahats.n51 <- round(data.frame(apply(many.thetahats.n51, 1, var)), digits = 2)
mse.thetahats.n51 <- round(data.frame(apply(many.thetahats.n51, 1, mse)), digits = 2)
```

 |                  |        $n=3$      |         $n=11$      |          $n=51$    | 
 | ----------------- | ----------------- | ------------------  | ------------------- |
 | $B(\hat{\theta}_1)$ | `r mean.thetahats.n3[1,]` |  `r mean.thetahats.n11[1,]` |`r mean.thetahats.n51[1,]` |
 | $B(\hat{\theta}_2)$ | `r mean.thetahats.n3[2,]` |  `r mean.thetahats.n11[2,]` | `r mean.thetahats.n51[2,]` |
 | $B(\hat{\theta}_3)$ | `r mean.thetahats.n3[3,]` | `r mean.thetahats.n11[3,]` | `r mean.thetahats.n51[3,]` |
 | ----------------- | ----------------- | ------------------  | ------------------- |
 | $Var(\hat{\theta}_1)$ | `r var.thetahats.n3[1,]` | `r var.thetahats.n11[1,]` | `r var.thetahats.n51[1,]` |
 | $Var(\hat{\theta}_2)$ | `r var.thetahats.n3[2,]` | `r var.thetahats.n11[2,]` | `r var.thetahats.n51[2,]` |
 | $Var(\hat{\theta}_3)$ | `r var.thetahats.n3[3,]` | `r var.thetahats.n11[3,]` | `r var.thetahats.n51[3,]` |
 ----------------- | ----------------- | ------------------  | -------------------
 | $MSE(\hat{\theta}_1)$ | `r mse.thetahats.n3[1,]` | `r mse.thetahats.n11[1,]` | `r mse.thetahats.n51[1,]` |
 | $MSE(\hat{\theta}_2)$ | `r mse.thetahats.n3[2,]` | `r mse.thetahats.n11[2,]` | `r mse.thetahats.n51[2,]` |
 | $MSE(\hat{\theta}_3)$ | `r mse.thetahats.n3[3,]` | `r mse.thetahats.n11[3,]` | `r mse.thetahats.n51[3,]` |    


5. Which estimator is best?  Justify your answer.

\textbf{Solution.}
Using the above table, we can see that as $n$ increases $\hat{\theta}_2$ is unbiased, has the smallest variance and the lowest $MSE.$ Thus I would say that $\hat{\theta}_2$ is the best estimator.

$\hfill\square$